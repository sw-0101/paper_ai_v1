{
    "Attention Is All You Need": {
        "references": [
            "Generating Sequences With Recurrent Neural Networks",
            "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "Neural Machine Translation by Jointly Learning to Align and Translate",
            "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
            "Effective Approaches to Attention-based Neural Machine Translation",
            "Neural Machine Translation of Rare Words with Subword Units",
            "Multi-task Sequence to Sequence Learning",
            "Rethinking the Inception Architecture for Computer Vision",
            "Long Short-Term Memory-Networks for Machine Reading",
            "Exploring the Limits of Language Modeling",
            "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
            "Layer Normalization",
            "Using the Output Embedding to Improve Language Models",
            "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
            "Xception: Deep Learning with Depthwise Separable Convolutions",
            "Neural Machine Translation in Linear Time",
            "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
            "A Structured Self-attentive Sentence Embedding",
            "Massive Exploration of Neural Machine Translation Architectures",
            "Factorization tricks for LSTM networks",
            "Convolutional Sequence to Sequence Learning",
            "A Deep Reinforced Model for Abstractive Summarization"
        ],
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "label": [
            "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "Using the Output Embedding to Improve Language Models",
            "A Structured Self-attentive Sentence Embedding"
        ]
    },
    "Neural Machine Translation by Jointly Learning to Align and Translate": {
        "references": [
            "ADADELTA: An Adaptive Learning Rate Method",
            "Generating Sequences With Recurrent Neural Networks"
        ],
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
        "label": [
            "ADADELTA: An Adaptive Learning Rate Method",
            "Generating Sequences With Recurrent Neural Networks"
        ]
    },
    "Segment Anything": {
        "references": [
            "Gaussian Error Linear Units (GELUs)",
            "Layer Normalization",
            "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
            "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
            "Quantifying the Carbon Emissions of Machine Learning",
            "Scaling Laws for Neural Language Models",
            "Getting to 99% Accuracy in Interactive Segmentation",
            "NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation",
            "TrashCan: A Semantically-Segmented Dataset towards Visual Detection of Marine Debris",
            "Carbon Emissions and Large Neural Network Training",
            "BEiT: BERT Pre-Training of Image Transformers",
            "On the Opportunities and Risks of Foundation Models",
            "iShape: A First Step Towards Irregular Shape Instance Segmentation",
            "Training Compute-Optimal Large Language Models",
            "PaLM: Scaling Language Modeling with Pathways",
            "SimpleClick: Interactive Image Segmentation with Simple Vision Transformers",
            "OneFormer: One Transformer to Rule Universal Image Segmentation"
        ],
        "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.",
        "label": [
            "On the Opportunities and Risks of Foundation Models",
            "NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation",
            "Getting to 99% Accuracy in Interactive Segmentation"
        ]
    },
    "Fast Segment Anything": {
        "references": [
            "YOLOX: Exceeding YOLO Series in 2021",
            "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
            "Segment Anything"
        ],
        "abstract": "The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.",
        "label": [
            "Segment Anything",
            "YOLOX: Exceeding YOLO Series in 2021",
            "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"
        ]
    },
    "Learning Transferable Visual Models From Natural Language Supervision": {
        "references": [
            "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild",
            "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
            "Deep Structured Output Learning for Unconstrained Text Recognition",
            "Explaining and Harnessing Adversarial Examples",
            "Adam: A Method for Stochastic Optimization",
            "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "Neural Machine Translation of Rare Words with Subword Units",
            "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
            "Training Deep Nets with Sublinear Memory Cost",
            "Gaussian Error Linear Units (GELUs)",
            "SGDR: Stochastic Gradient Descent with Warm Restarts",
            "HyperNetworks",
            "Learning through Dialogue Interactions by Asking Questions",
            "ShapeWorld - A new test methodology for multimodal language understanding",
            "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives",
            "Mixed Precision Training",
            "Learning with Latent Language",
            "Decoupled Weight Decay Regularization",
            "Deep Learning Scaling is Predictable, Empirically",
            "Universal Language Model Fine-tuning for Text Classification",
            "Generating Wikipedia by Summarizing Long Sequences",
            "Deep contextualized word representations",
            "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
            "Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination",
            "The Natural Language Decathlon: Multitask Learning as Question Answering",
            "Representation Learning with Contrastive Predictive Coding",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
            "EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification",
            "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!",
            "Learning and Evaluating General Linguistic Intelligence",
            "Do ImageNet Classifiers Generalize to ImageNet?",
            "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
            "Making Convolutional Networks Shift-Invariant Again",
            "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "Do Image Classifiers Generalize Across Time?",
            "Contrastive Multiview Coding",
            "A Short Note on the Kinetics-700 Human Action Dataset",
            "Natural Adversarial Examples",
            "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "UNITER: UNiversal Image-TExt Representation Learning",
            "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
            "On Empirical Comparisons of Optimizers for Deep Learning",
            "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "Shaping Visual Representations with Language for Few-shot Classification",
            "Exposing and Correcting the Gender Bias in Image Captioning Datasets and Models",
            "Big Transfer (BiT): General Visual Representation Learning",
            "SciPy 1.0: fundamental algorithms for scientific computing in Python",
            "Array programming with NumPy",
            "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
            "Scaling Laws for Neural Language Models",
            "Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods",
            "A Simple Framework for Contrastive Learning of Visual Representations",
            "Improved Baselines with Momentum Contrastive Learning",
            "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?",
            "Pretrained Transformers Improve Out-of-Distribution Robustness",
            "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
            "Shortcut Learning in Deep Neural Networks",
            "The Effect of Natural Distribution Shift on Question Answering Models",
            "Jukebox: A Generative Model for Music",
            "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
            "ExpBERT: Representation Engineering with Natural Language Explanations",
            "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes",
            "Language Models are Few-Shot Learners",
            "Large-Scale Adversarial Training for Vision-and-Language Representation Learning",
            "VirTex: Learning Visual Representations from Textual Annotations",
            "Bootstrap your own latent: A new approach to self-supervised Learning",
            "Big Self-Supervised Models are Strong Semi-Supervised Learners",
            "Self-Supervised MultiModal Versatile Networks",
            "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
            "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph",
            "Measuring Robustness to Natural Distribution Shifts in Image Classification",
            "Learning Video Representations from Textual Web Supervision",
            "RareAct: A video dataset of unusual interactions",
            "Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition",
            "ALICE: Active Learning with Contrastive Natural Language Explanations",
            "Contrastive Learning of Medical Visual Representations from Paired Images and Text",
            "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation",
            "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
            "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption",
            "A Multimodal Framework for the Detection of Hateful Memes",
            "Making Pre-trained Language Models Better Few-shot Learners"
        ],
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
        "label": [
            "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
            "Contrastive Learning of Medical Visual Representations from Paired Images and Text",
            "A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation"
        ]
    }
}
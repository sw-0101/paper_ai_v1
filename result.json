{
    "Attention Is All You Need": {
        "references": [
            "Generating Sequences With Recurrent Neural Networks",
            "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "Neural Machine Translation by Jointly Learning to Align and Translate",
            "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
            "Effective Approaches to Attention-based Neural Machine Translation",
            "Neural Machine Translation of Rare Words with Subword Units",
            "Multi-task Sequence to Sequence Learning",
            "Rethinking the Inception Architecture for Computer Vision",
            "Long Short-Term Memory-Networks for Machine Reading",
            "Exploring the Limits of Language Modeling",
            "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
            "Layer Normalization",
            "Using the Output Embedding to Improve Language Models",
            "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
            "Xception: Deep Learning with Depthwise Separable Convolutions",
            "Neural Machine Translation in Linear Time",
            "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
            "A Structured Self-attentive Sentence Embedding",
            "Massive Exploration of Neural Machine Translation Architectures",
            "Factorization tricks for LSTM networks",
            "Convolutional Sequence to Sequence Learning",
            "A Deep Reinforced Model for Abstractive Summarization"
        ],
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "label": [
            "Long Short-Term Memory-Networks for Machine Reading",
            "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "Layer Normalization"
        ]
    },
    "Neural Machine Translation by Jointly Learning to Align and Translate": {
        "references": [
            "ADADELTA: An Adaptive Learning Rate Method",
            "Generating Sequences With Recurrent Neural Networks"
        ],
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
        "label": [
            "ADADELTA: An Adaptive Learning Rate Method",
            "Generating Sequences With Recurrent Neural Networks"
        ]
    },
    "Segment Anything": {
        "references": [
            "Gaussian Error Linear Units (GELUs)",
            "Layer Normalization",
            "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
            "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
            "Quantifying the Carbon Emissions of Machine Learning",
            "Scaling Laws for Neural Language Models",
            "Getting to 99% Accuracy in Interactive Segmentation",
            "NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation",
            "TrashCan: A Semantically-Segmented Dataset towards Visual Detection of Marine Debris",
            "Carbon Emissions and Large Neural Network Training",
            "BEiT: BERT Pre-Training of Image Transformers",
            "On the Opportunities and Risks of Foundation Models",
            "iShape: A First Step Towards Irregular Shape Instance Segmentation",
            "Training Compute-Optimal Large Language Models",
            "PaLM: Scaling Language Modeling with Pathways",
            "SimpleClick: Interactive Image Segmentation with Simple Vision Transformers",
            "OneFormer: One Transformer to Rule Universal Image Segmentation"
        ],
        "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.",
        "label": [
            "BEiT: BERT Pre-Training of Image Transformers",
            "On the Opportunities and Risks of Foundation Models",
            "SimpleClick: Interactive Image Segmentation with Simple Vision Transformers"
        ]
    },
    "Fast Segment Anything": {
        "references": [
            "YOLOX: Exceeding YOLO Series in 2021",
            "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
            "Segment Anything"
        ],
        "abstract": "The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.",
        "label": [
            "YOLOX: Exceeding YOLO Series in 2021",
            "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
            "Segment Anything"
        ]
    },
    "Learning Transferable Visual Models From Natural Language Supervision": {
        "references": [
            "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild",
            "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
            "Deep Structured Output Learning for Unconstrained Text Recognition",
            "Explaining and Harnessing Adversarial Examples",
            "Adam: A Method for Stochastic Optimization",
            "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "Neural Machine Translation of Rare Words with Subword Units",
            "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
            "Training Deep Nets with Sublinear Memory Cost",
            "Gaussian Error Linear Units (GELUs)",
            "SGDR: Stochastic Gradient Descent with Warm Restarts",
            "HyperNetworks",
            "Learning through Dialogue Interactions by Asking Questions",
            "ShapeWorld - A new test methodology for multimodal language understanding",
            "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives",
            "Mixed Precision Training",
            "Learning with Latent Language",
            "Decoupled Weight Decay Regularization",
            "Deep Learning Scaling is Predictable, Empirically",
            "Universal Language Model Fine-tuning for Text Classification",
            "Generating Wikipedia by Summarizing Long Sequences",
            "Deep contextualized word representations",
            "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
            "Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination",
            "The Natural Language Decathlon: Multitask Learning as Question Answering",
            "Representation Learning with Contrastive Predictive Coding",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
            "EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification",
            "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!",
            "Learning and Evaluating General Linguistic Intelligence",
            "Do ImageNet Classifiers Generalize to ImageNet?",
            "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
            "Making Convolutional Networks Shift-Invariant Again",
            "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "Do Image Classifiers Generalize Across Time?",
            "Contrastive Multiview Coding",
            "A Short Note on the Kinetics-700 Human Action Dataset",
            "Natural Adversarial Examples",
            "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
            "UNITER: UNiversal Image-TExt Representation Learning",
            "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
            "On Empirical Comparisons of Optimizers for Deep Learning",
            "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "Shaping Visual Representations with Language for Few-shot Classification",
            "Exposing and Correcting the Gender Bias in Image Captioning Datasets and Models",
            "Big Transfer (BiT): General Visual Representation Learning",
            "SciPy 1.0: fundamental algorithms for scientific computing in Python",
            "Array programming with NumPy",
            "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
            "Scaling Laws for Neural Language Models",
            "Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods",
            "A Simple Framework for Contrastive Learning of Visual Representations",
            "Improved Baselines with Momentum Contrastive Learning",
            "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?",
            "Pretrained Transformers Improve Out-of-Distribution Robustness",
            "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
            "Shortcut Learning in Deep Neural Networks",
            "The Effect of Natural Distribution Shift on Question Answering Models",
            "Jukebox: A Generative Model for Music",
            "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
            "ExpBERT: Representation Engineering with Natural Language Explanations",
            "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes",
            "Language Models are Few-Shot Learners",
            "Large-Scale Adversarial Training for Vision-and-Language Representation Learning",
            "VirTex: Learning Visual Representations from Textual Annotations",
            "Bootstrap your own latent: A new approach to self-supervised Learning",
            "Big Self-Supervised Models are Strong Semi-Supervised Learners",
            "Self-Supervised MultiModal Versatile Networks",
            "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
            "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph",
            "Measuring Robustness to Natural Distribution Shifts in Image Classification",
            "Learning Video Representations from Textual Web Supervision",
            "RareAct: A video dataset of unusual interactions",
            "Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition",
            "ALICE: Active Learning with Contrastive Natural Language Explanations",
            "Contrastive Learning of Medical Visual Representations from Paired Images and Text",
            "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation",
            "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
            "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption",
            "A Multimodal Framework for the Detection of Hateful Memes",
            "Making Pre-trained Language Models Better Few-shot Learners"
        ],
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
        "label": ["ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
                  "Making Pre-trained Language Models Better Few-shot Learners",
                  "Pretrained Transformers Improve Out-of-Distribution Robustness"
                ]
    },
    "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm": {
        "references": [],
        "abstract": "The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",
        "label": []
    },
    "The Sequence of the Human Genome": {
        "references": [
            "Nucleotide sequence of bacteriophage \u03c6X174 DNA",
            "DNA Sequencing with Chain-Terminating Inhibitors",
            "Genetic Expression in the Developing Brain",
            "On the equality of origin and fixation times in genetics",
            "Primary Structure of Rat Cardiac \u03b2 -Adrenergic and Muscarinic Cholinergic Receptors Obtained by Automated DNA Sequence Analysis: Further Evidence for a Multigene Family",
            "A System for Rapid DNA Sequencing with Fluorescent Chain-Terminating Dideoxynucleotides",
            "Inferring statistical complexity",
            "A human nuclear uracil DNA glycosylase is the 37-kDa subunit of glyceraldehyde-3-phosphate dehydrogenase.",
            "Complementary DNA Sequencing: Expressed Sequence Tags and Human Genome Project",
            "Cloning and Expression of the cDNA for Human \u03b3-Glytamyl Carboxylase",
            "Sequence identification of 2,375 human brain genes",
            "Number of CpG islands and genes in human and mouse.",
            "A model for high-throughput automated DNA sequencing and analysis core facilities",
            "Whole-Genome Random Sequencing and Assembly of Haemophilus Influenzae Rd",
            "The Minimal Gene Complement of Mycoplasma genitalium",
            "Information measures, effective complexity, and total information",
            "A new strategy for genome sequencing",
            "Complete Genome Sequence of the Methanogenic Archaeon, Methanococcus jannaschii",
            "Life with 6000 Genes",
            "The complete genome sequence of the hyperthermophilic, sulphate-reducing archaeon Archaeoglobus fulgidus",
            "An ancient retrotransposal insertion causes Fukuyama-type congenital muscular dystrophy",
            "Data Transferability from Model Organisms to Human Beings: Insights from the Functional Genomics of the Flightless Region of Drosophila",
            "The Lethal Mutation of the Mouse Wasted (wst) is a Deletion that Abolishes Expression of a Tissue-Specific Isoform of Translation Elongation Factor 1\u03b1 , Encoded by the Eefla2 Gene",
            "SEQUENCING: Hubris and the Human Genome",
            "Large-Scale Identification, Mapping, and Genotyping of Single-Nucleotide Polymorphisms in the Human Genome",
            "HUMAN GENOME PROJECT: Funders Reassure Genome Sequencers",
            "GENOMICS: Shotgun Sequencing of the Human Genome",
            "Complete Genome Sequence of Treponema pallidum, the Syphilis Spirochete",
            "New Goals for the U.S. Human Genome Project: 1998-2003",
            "A Physical Map of 30,000 Human Genes",
            "Genome Sequence of the Nematode C. elegans: A Platform for Investigating Biology",
            "Comparison of the Complete Protein Sets of Worm and Yeast: Orthology",
            "Gene estimate rises as US and UK discuss freedom of access",
            "The dynamics of chromosome evolution in birds and mammals",
            "The DNA sequence of human chromosome 22",
            "Sequence and analysis of chromosome 2 of the plant Arabidopsis thaliana",
            "Emergence of Scaling in Random Networks",
            "Specificities of heparan sulphate proteoglycans in developmental processes",
            "The DNA sequence of human chromosome 21",
            "An SNP map of the human genome generated by reduced representation shotgun sequencing",
            "Regenerating the damaged central nervous system",
            "Signal-dependent nuclear export of a histone deacetylase regulates muscle differentiation",
            "Analysis of the genome sequence of the flowering plant Arabidopsis thaliana",
            "Direct interaction between synaptotagmin and the intracellular loop I-II of neuronal voltage-sensitive sodium channels",
            "Y chromosomal fertility factors kl-2 and kl-3 of Drosophila melanogaster encode dynein heavy chain polypeptides",
            "Complete Genome Sequence of Neisseria meningitidis Serogroup B Strain MC58",
            "The Genome Sequence of Drosophila melanogaster",
            "A Whole-Genome Assembly of Drosophila",
            "Comparative Genomics of the Eukaryotes",
            "Requirement of the RNA Editing Deaminase ADAR1 Gene for Embryonic Erythropoiesis",
            "Arabidopsis Transcription Factors: Genome-Wide Comparative Analysis Among Eukaryotes",
            "Initial sequencing and analysis of the human genome",
            "A High-Resolution Radiation Hybrid Map of the Human Genome Draft Sequence"
        ],
        "abstract": "A 2.91-billion base pair (bp) consensus sequence of the euchromatic portion of the human genome was generated by the whole-genome shotgun sequencing method. The 14.8-billion bp DNA sequence was generated over 9 months from 27,271,853 high-quality sequence reads (5.11-fold coverage of the genome) from both ends of plasmid clones made from the DNA of five individuals. Two assembly strategies-a whole-genome assembly and a regional chromosome assembly-were used, each combining sequence data from Celera and the publicly funded genome effort. The public data were shredded into 550-bp segments to create a 2.9-fold coverage of those genome regions that had been sequenced, without including biases inherent in the cloning and assembly procedure used by the publicly funded group. This brought the effective coverage in the assemblies to eightfold, reducing the number and size of gaps in the final assembly over what would be obtained with 5.11-fold coverage. The two assembly strategies yielded very similar results that largely agree with independent mapping data. The assemblies effectively cover the euchromatic regions of the human chromosomes. More than 90% of the genome is in scaffold assemblies of 100,000 bp or more, and 25% of the genome is in scaffolds of 10 million bp or larger. Analysis of the genome sequence revealed 26,588 protein-encoding transcripts for which there was strong corroborating evidence and an additional ~12,000 computationally derived genes with mouse matches or other weak supporting evidence. Although gene-dense clusters are obvious, almost half the genes are dispersed in low G+C sequence separated by large tracts of apparently noncoding sequence. Only 1.1% of the genome is spanned by exons, whereas 24% is in introns, with 75% of the genome being intergenic DNA. Duplications of segmental blocks, ranging in size up to chromosomal lengths, are abundant throughout the genome and reveal a complex evolutionary history. Comparative genomic analysis indicates vertebrate expansions of genes associated with neuronal function, with tissue-specific developmental regulation, and with the hemostasis and immune systems. DNA sequence comparisons between the consensus sequence and publicly funded genome data provided locations of 2.1 million single-nucleotide polymorphisms (SNPs). A random pair of human haploid genomes differed at a rate of 1 bp per 1250 on average, but there was marked heterogeneity in the level of polymorphism across the genome. Less than 1% of all SNPs resulted in variation in proteins, but the task of determining which SNPs have functional consequences remains an open challenge.",
        "label": []
    },
    "Observation of Gravitational Waves from a Binary Black Hole Merger": {
        "references": [
            "N\u00e4herungsweise Integration der Feldgleichungen der Gravitation",
            "\u00dcber Gravitationswellen",
            "Past-Future Asymmetry of the Gravitational Field of a Point Particle",
            "Detection and Generation of Gravitational Waves",
            "Maximal Extension of Schwarzschild Metric",
            "Gravitational Field of a Spinning Mass as an Example of Algebraically Special Metrics",
            "Scattering of Gravitational Radiation by a Schwarzschild Black-hole",
            "Long Wave Trains of Gravitational Waves from a Vibrating Black Hole",
            "Photon-noise-limited laser transducer for gravitational antenna.",
            "Gravitational-Wave Astronomy",
            "Dimensions of the Binary System HDE 226868 = Cygnus X-1",
            "Cygnus X-1-a Spectroscopic Binary with a Heavy Companion ?",
            "Evolution of massive close binaries",
            "Mass of the graviton",
            "Discovery of a pulsar in a binary system.",
            "The Quasi-Normal Modes of the Schwarzschild Black Hole",
            "A new test of general relativity - Gravitational radiation and the binary pulsar PSR 1913+16",
            "Coalescing binaries\u2014Probe of the universe",
            "Recycling in laser-interferometric gravitational-wave detectors",
            "Model-independent constraints on possible modifications of Newtonian gravity",
            "Choice of filters for the detection of gravitational waves from coalescing binaries",
            "LIGO: The Laser Interferometer Gravitational-Wave Observatory",
            "Primordial black holes in globular clusters",
            "Resonant sideband extraction: a new configuration for interferometric gravitational wave detectors",
            "Gravitational-Radiation Damping of Compact Binary Systems to Second Post-Newtonian Order",
            "Formation and coalescence of relativistic binary stars: the effect of kick velocity",
            "Bounding the mass of the graviton using gravitational-wave observations of inspiralling compact binaries",
            "Effective one-body approach to general relativistic two-body dynamics",
            "Matched filtering of gravitational waves from inspiraling compact binaries: Computational cost and template placement",
            "Black Hole Mergers in the Universe",
            "Transition from inspiral to plunge in binary black hole coalescences",
            "Bounding the mass of the graviton using binary pulsar observations",
            "The Probability Distribution of Binary Pulsar Coalescence Rates. I. Double Neutron Star Systems in the Galactic Field",
            "Multiresolution techniques for the detection of gravitational-wave bursts",
            "Gravitational Radiation from Inspiralling Compact Binaries Completed at the Third Post-Newtonian Order",
            "\u03c7<SUP>2</SUP> time-frequency discriminator for gravitational wave detection",
            "Evolution of Binary Black-Hole Spacetimes",
            "Accurate Evolutions of Orbiting Black-Hole Binaries without Excision",
            "Gravitational-Wave Extraction from an Inspiraling Configuration of Merging Black Holes",
            "Titania-doped tantala/silica coatings for gravitational-wave detection",
            "A coherent method for detection of gravitational wave bursts",
            "On the Maximum Mass of Stellar Black Holes",
            "Accurate calibration of test mass displacement in the LIGO interferometers",
            "The Advanced LIGO timing system",
            "Alignment sensing and control in advanced LIGO",
            "TOPICAL REVIEW:  Predictions for the rates of compact binary coalescences observable by ground-based gravitational-wave detectors",
            "IGEC2: A 17-month search for gravitational wave bursts in 2005-2007",
            "Parametrized tests of post-Newtonian theory using Advanced LIGO and Einstein Telescope",
            "Josh Goldberg and the physical reality of gravitational waves",
            "Toward Early-warning Detection of Gravitational Waves from Compact Binary Coalescence",
            "Design and development of the advanced LIGO monolithic fused silica suspension",
            "DC readout experiment in Enhanced LIGO",
            "Update on quadruple suspension design for Advanced LIGO",
            "Stabilized high-power laser system for the gravitational wave detector advanced LIGO",
            "Search for gravitational waves from low mass compact binary coalescence in LIGO's sixth science run and Virgo's science runs 2 and 3",
            "Towards a generic test of the strong field dynamics of general relativity using compact binary coalescence",
            "FINDCHIRP: An algorithm for detection of gravitational waves from inspiraling compact binaries",
            "Search for gravitational waves from binary black hole inspiral, merger, and ringdown in LIGO-Virgo data from 2009-2010",
            "Searching for gravitational waves from binary coalescence",
            "Interferometer design of the KAGRA gravitational wave detector",
            "Catalog of 174 Binary Black Hole Simulations for Gravitational Wave Astronomy",
            "Frequency-domain reduced order models for gravitational waves from aligned-spin compact binaries",
            "Advanced techniques in GEO 600",
            "Achieving resonance in the Advanced LIGO gravitational-wave interferometer",
            "A step toward exploring the features of Gravidilaton sector in Randall-Sundrum scenario via lightest Kaluza-Klein graviton mass",
            "Gravitational Radiation from Post-Newtonian Sources and Inspiralling Compact Binaries",
            "Improving the sensitivity of a search for coalescing binary black holes with nonprecessing spins in gravitational wave data",
            "Effective-one-body model for black-hole binaries with generic mass ratios and spins",
            "Remnant mass, spin, and recoil from spin aligned black-hole binaries",
            "Simple Model of Complete Precessing Black-Hole-Binary Gravitational Waveforms",
            "Mass Measurements of Stellar and Intermediate-Mass Black Holes",
            "Advanced Virgo: a second-generation interferometric gravitational wave detector",
            "Environmental influences on the LIGO gravitational wave detectors during the 6th science run",
            "Advanced LIGO",
            "Characterization of the LIGO detectors during their sixth science run",
            "Bayeswave: Bayesian inference for gravitational wave bursts and instrument glitches",
            "Seismic isolation of Advanced LIGO: Review of strategy, instrumentation and performance",
            "Improving the data quality of Advanced LIGO based on early engineering run results",
            "The mass spectrum of compact remnants from the PARSEC stellar evolution tracks",
            "Counting and confusion: Bayesian rate estimation with multiple populations",
            "Parameter estimation for compact binaries with ground-based gravitational-wave observations using the LALInference software library",
            "Binary Black Hole Mergers from Globular Clusters: Implications for Advanced LIGO",
            "Planck 2015 results. XIII. Cosmological parameters",
            "Compact Binary Merger Rates: Comparison with LIGO/Virgo Upper Limits",
            "The PyCBC search for gravitational waves from compact binary coalescence",
            "Prospects for Observing and Localizing Gravitational-Wave Transients with Advanced LIGO and Advanced Virgo",
            "Mechanical loss in state-of-the-art amorphous optical coatings",
            "Leveraging waveform complexity for confident detection of gravitational waves",
            "Method for detection and reconstruction of gravitational wave transients with networks of advanced detectors",
            "Frequency-domain gravitational waves from nonprecessing black-hole binaries. I. New numerical waveforms and anatomy of the signal",
            "Frequency-domain gravitational waves from nonprecessing black-hole binaries. II. A phenomenological model for the advanced detector era",
            "The advanced LIGO input optics",
            "Information-theoretic approach to the gravitational-wave burst detection problem"
        ],
        "abstract": "On September 14, 2015 at 09:50:45 UTC the two detectors of the Laser Interferometer Gravitational-Wave Observatory simultaneously observed a transient gravitational-wave signal. The signal sweeps upwards in frequency from 35 to 250 Hz with a peak gravitational-wave strain of 1.0 \u00d710<SUP>-21</SUP>. It matches the waveform predicted by general relativity for the inspiral and merger of a pair of black holes and the ringdown of the resulting single black hole. The signal was observed with a matched-filter signal-to-noise ratio of 24 and a false alarm rate estimated to be less than 1 event per 203 000 years, equivalent to a significance greater than 5.1 \u03c3 . The source lies at a luminosity distance of 41 0<SUB>-180</SUB><SUP>+160</SUP> Mpc corresponding to a redshift z =0.0 9<SUB>-0.04</SUB><SUP>+0.03</SUP> . In the source frame, the initial black hole masses are 3 6<SUB>-4</SUB><SUP>+5</SUP>M<SUB>\u2299</SUB> and 2 9<SUB>-4</SUB><SUP>+4</SUP>M<SUB>\u2299</SUB> , and the final black hole mass is 6 2<SUB>-4</SUB><SUP>+4</SUP>M<SUB>\u2299</SUB> , with 3. 0<SUB>-0.5</SUB><SUP>+0.5</SUP>M<SUB>\u2299</SUB> c<SUP>2</SUP> radiated in gravitational waves. All uncertainties define 90% credible intervals. These observations demonstrate the existence of binary stellar-mass black hole systems. This is the first direct detection of gravitational waves and the first observation of a binary black hole merger.",
        "label": []
    },
    "Fractional-moment CAPM with loss aversion": {
        "references": [
            "Scale relativity and fractal space-time: applications to quantum physics, cosmology and chaotic systems.",
            "Scale-relativity and quantization of the universe. I. Theoretical framework",
            "New evidence for the power-law distribution of wealth",
            "The scale-relativity program",
            "Entwined paths, difference equations, and the Dirac equation",
            "A review of E infinity theory and the mass spectrum of high energy particle physics",
            "Waveguiding and mirroring effects in stochastic self-similar and Cantorian \u025b <SUP>(\u221e)</SUP> universe",
            "On Penrose view of transfinite sets and computability and the fractal character of E-infinity spacetime",
            "Elementary prerequisites for E-infinity . (Recommended background readings in nonlinear dynamics, geometry and topology)",
            "Intermediate prerequisites for E-infinity theory (Further recommended reading in nonlinear dynamics and mathematical physics)",
            "Advanced prerequisite for E-infinity theory",
            "The Fibonacci code behind super strings and P-Branes. An answer to M. Kaku's fundamental question",
            "Lagrangian mechanics of fractional order, Hamilton-Jacobi fractional PDE and Taylor's series of nondifferentiable functions",
            "From Lagrangian mechanics fractal in space to space fractal Schr\u00f6dinger's equation via fractional Taylor's series"
        ],
        "abstract": "In this paper, we present a new fractional-order value function which generalizes the value function of Kahneman and Tversky [Kahneman D, Tversky A. Prospect theory: an analysis of decision under risk. Econometrica 1979;47:263-91; Tversky A, Kahneman D. Advances in prospect theory: cumulative representation of uncertainty. J. Risk Uncertainty 1992;4:297-323], and give the corresponding fractional-moment versions of CAPM in the cases of both the prospect theory [Kahneman D, Tversky A. Prospect theory: an analysis of decision under risk. Econometrica 1979;47:263-91; Tversky A, Kahneman D. Advances in prospect theory: cumulative representation of uncertainty. J. Risk Uncertainty 1992;4:297-323] and the expected utility model. The models that we obtain can be used to price assets when asset return distributions are likely to be asymmetric stable Levy distribution during panics and stampedes in worldwide security markets in 2008. In particular, from the prospect theory we get the following fractional-moment CAPM with loss aversion: E (R<SUB>i</SUB> - R<SUB>0</SUB>) = E [ (W - W<SUB>0</SUB> )<SUB>+</SUB><SUP>-</SUP> 0.12 (R<SUB>i</SUB> - R<SUB>0</SUB>) ] + 2.25 E [ (W<SUB>0</SUB> - W )<SUB>+</SUB><SUP>-</SUP> 0.12 (R<SUB>i</SUB> - R<SUB>0</SUB>) ]/E [ (W - W<SUB>0</SUB> )<SUB>+</SUB><SUP>-</SUP> 0.12 (W - R<SUB>0</SUB>) ] + 2.25 E [ (W<SUB>0</SUB> - W )<SUB>+</SUB><SUP>-</SUP> 0.12 (W - R<SUB>0</SUB>) ] \u00b7 E (W - R<SUB>0</SUB>) , <P />where W<SUB>0</SUB> is a fixed reference point distinguishing between losses and gains.",
        "label": []
    }
}